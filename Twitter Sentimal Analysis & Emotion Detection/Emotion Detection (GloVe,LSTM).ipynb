{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4771d321",
   "metadata": {},
   "source": [
    "# Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35bbb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd93777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv',encoding= 'latin-1',header = None) ### download data from https://www.kaggle.com/kazanova/sentiment140\n",
    "df = df.sample(frac = 1) ### shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "214d1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={0: 'target', 1: 'id', 2: 'date', 3: 'query', 4: 'username', 5: 'content'}) # add names for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b21cf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1600000 entries, 237705 to 633129\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   target    1600000 non-null  int64 \n",
      " 1   id        1600000 non-null  int64 \n",
      " 2   date      1600000 non-null  object\n",
      " 3   query     1600000 non-null  object\n",
      " 4   username  1600000 non-null  object\n",
      " 5   content   1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 85.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info()) # check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3573e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id','date','query','username'],axis=1) # drop unimportant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86292dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### in target column 0 is unhappy and 4 is happy ###\n",
    "### here just replaced 4 with 1 just to make more sense ###\n",
    "df.target = df.target.replace({4:1}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1dcff9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mahmo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "#stop-words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6231b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "regex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "# the upper regex can't detect urls starting with www. but can detect mentions the lower one can detect urls starting with www. but can't detect mentions\n",
    "# regex => (http://)[^ ]*|(https://)[^ ]*|(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\n",
    "\n",
    "def preprocess(content, stem=False):\n",
    "  content = re.sub(regex, ' ', str(content).lower()).strip()\n",
    "  tokens = []\n",
    "  for token in content.split():\n",
    "    tokens.append(stemmer.stem(token))\n",
    "  return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d4f5449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237705</th>\n",
       "      <td>0</td>\n",
       "      <td>hug take it easi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450282</th>\n",
       "      <td>0</td>\n",
       "      <td>i use to have eo manip but i threw them away w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820399</th>\n",
       "      <td>1</td>\n",
       "      <td>yea i just need to put my dress on and stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049909</th>\n",
       "      <td>1</td>\n",
       "      <td>i like those word of wisdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646622</th>\n",
       "      <td>0</td>\n",
       "      <td>i want corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623656</th>\n",
       "      <td>0</td>\n",
       "      <td>164 with ship he is awesom just suck i ve had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368004</th>\n",
       "      <td>0</td>\n",
       "      <td>not look forward to tmw at present to open my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                            content\n",
       "237705        0                                   hug take it easi\n",
       "450282        0  i use to have eo manip but i threw them away w...\n",
       "820399        1       yea i just need to put my dress on and stuff\n",
       "1049909       1                        i like those word of wisdom\n",
       "646622        0                                      i want corona\n",
       "623656        0  164 with ship he is awesom just suck i ve had ...\n",
       "368004        0  not look forward to tmw at present to open my ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content = df.content.apply(lambda x: preprocess(x))\n",
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22315177",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fcc414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T11:09:57.821477Z",
     "iopub.status.busy": "2021-02-26T11:09:57.820623Z",
     "iopub.status.idle": "2021-02-26T11:09:58.071476Z",
     "shell.execute_reply": "2021-02-26T11:09:58.070873Z"
    },
    "papermill": {
     "duration": 0.33686,
     "end_time": "2021-02-26T11:09:58.071679",
     "exception": false,
     "start_time": "2021-02-26T11:09:57.734819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(data, test_size=0.1, random_state=44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5bbc8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T11:09:58.237321Z",
     "iopub.status.busy": "2021-02-26T11:09:58.236323Z",
     "iopub.status.idle": "2021-02-26T11:09:58.241248Z",
     "shell.execute_reply": "2021-02-26T11:09:58.240679Z"
    },
    "papermill": {
     "duration": 0.090259,
     "end_time": "2021-02-26T11:09:58.241405",
     "exception": false,
     "start_time": "2021-02-26T11:09:58.151146",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (1440000, 2)\n",
      "Test dataset shape: (160000, 2)\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset shape: {}'.format(train.shape))\n",
    "print('Test dataset shape: {}'.format(test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2a330",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxWordLength(content):\n",
    "    max_len = 0\n",
    "    for doc in data:\n",
    "        for i in doc.split():\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "        max_len = max(max_len, len(doc))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b36ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train.content)  \n",
    "vocab_size = len(tokenizer.word_index) + 1 \n",
    "max_length = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87915b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences_train = tokenizer.texts_to_sequences(train.content) \n",
    "sequences_test = tokenizer.texts_to_sequences(test.content) \n",
    "\n",
    "X_train = pad_sequences(sequences_train, maxlen=max_length, padding='post')\n",
    "X_test = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n",
    "\n",
    "y_train = train.target.values\n",
    "y_test = test.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005a17e",
   "metadata": {},
   "source": [
    "# Word Embedding using Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a02ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxWordLength(data):\n",
    "    max_len = 0\n",
    "    for doc in data:\n",
    "        for i in doc.split():\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "        max_len = max(max_len, len(doc))\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.100d.txt') ### the file is large so you can download it from https://nlp.stanford.edu/projects/glove/ or search for it on google\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary [word] = vector_dimensions\n",
    "    \n",
    "glove_file.close()\n",
    "\n",
    "embedding_dim = 100 # embeddings_dictionary[any Existent word].shape[0]\n",
    "embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599279a2",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe0876df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#numpy\n",
    "import matplotlib.pyplot as plt\n",
    "#matplotlib\n",
    "import seaborn as sns\n",
    "#seaborn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "#sklearn\n",
    "import tensorflow as tf\n",
    "#tensorflow\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "#keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False)\n",
    "num_epochs = 10\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39dc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        embedding_layer,\n",
    "        tf.keras.layers.Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Bidirectional(LSTM(128)),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb859474",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(learning_rate=0.001), metrics=['accuracy'])\n",
    "history = model.fit(X_train, y_train, batch_size = batch_size, epochs=num_epochs, validation_data=(X_test, y_test), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab396601",
   "metadata": {},
   "source": [
    "# Evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05431143",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeefe0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#History for accuracy\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train accuracy', 'Test accuracy'], loc='lower right')\n",
    "plt.show()\n",
    "# History for loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Test loss'], loc='upper right')\n",
    "plt.suptitle('Accuracy and loss for second model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
