{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4771d321",
   "metadata": {},
   "source": [
    "# Data Preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "35bbb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "nltk.download('stopwords')\n",
    "#stop-words\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd93777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',encoding= 'latin-1', header = None) ### download data from https://www.kaggle.com/kazanova/sentiment140\n",
    "df = df.sample(frac = 1) ### shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "214d1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={0: 'target', 1: 'id', 2: 'date', 3: 'query', 4: 'username', 5: 'content'}) # add names for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b21cf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1600000 entries, 237705 to 633129\n",
      "Data columns (total 6 columns):\n",
      " #   Column    Non-Null Count    Dtype \n",
      "---  ------    --------------    ----- \n",
      " 0   target    1600000 non-null  int64 \n",
      " 1   id        1600000 non-null  int64 \n",
      " 2   date      1600000 non-null  object\n",
      " 3   query     1600000 non-null  object\n",
      " 4   username  1600000 non-null  object\n",
      " 5   content   1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 85.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info()) # check for nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3573e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id','date','query','username'],axis=1) # drop unimportant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86292dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### in target column 0 is unhappy and 4 is happy ###\n",
    "### here just replaced 4 with 1 just to make more sense ###\n",
    "df.target = df.target.replace({4:1}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6231b470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))\n",
    "stemmer = SnowballStemmer('english')\n",
    "regex = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n",
    "# the upper regex can't detect urls starting with www. but can detect mentions the lower one can detect urls starting with www. but can't detect mentions\n",
    "# regex => (http://)[^ ]*|(https://)[^ ]*|(https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|www\\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\\.[^\\s]{2,}|https?:\\/\\/(?:www\\.|(?!www))[a-zA-Z0-9]+\\.[^\\s]{2,}|www\\.[a-zA-Z0-9]+\\.[^\\s]{2,})\n",
    "\n",
    "def preprocess(content, stem=True):\n",
    "  content = re.sub(regex, ' ', str(content).lower()).strip()\n",
    "  tokens = []\n",
    "  for token in content.split():\n",
    "    if stem:\n",
    "      tokens.append(stemmer.stem(token))\n",
    "    else:\n",
    "      tokens.append(token)\n",
    "  return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3d4f5449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237705</th>\n",
       "      <td>0</td>\n",
       "      <td>hug take it easi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450282</th>\n",
       "      <td>0</td>\n",
       "      <td>i use to have eo manip but i threw them away w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820399</th>\n",
       "      <td>1</td>\n",
       "      <td>yea i just need to put my dress on and stuff</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049909</th>\n",
       "      <td>1</td>\n",
       "      <td>i like those word of wisdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>646622</th>\n",
       "      <td>0</td>\n",
       "      <td>i want corona</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623656</th>\n",
       "      <td>0</td>\n",
       "      <td>164 with ship he is awesom just suck i ve had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368004</th>\n",
       "      <td>0</td>\n",
       "      <td>not look forward to tmw at present to open my ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         target                                            content\n",
       "237705        0                                   hug take it easi\n",
       "450282        0  i use to have eo manip but i threw them away w...\n",
       "820399        1       yea i just need to put my dress on and stuff\n",
       "1049909       1                        i like those word of wisdom\n",
       "646622        0                                      i want corona\n",
       "623656        0  164 with ship he is awesom just suck i ve had ...\n",
       "368004        0  not look forward to tmw at present to open my ..."
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content = df.content.apply(lambda x: preprocess(x))\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22315177",
   "metadata": {},
   "source": [
    "# train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c865b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fcc414",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-26T11:09:57.821477Z",
     "iopub.status.busy": "2021-02-26T11:09:57.820623Z",
     "iopub.status.idle": "2021-02-26T11:09:58.071476Z",
     "shell.execute_reply": "2021-02-26T11:09:58.070873Z"
    },
    "papermill": {
     "duration": 0.33686,
     "end_time": "2021-02-26T11:09:58.071679",
     "exception": false,
     "start_time": "2021-02-26T11:09:57.734819",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.25, random_state=77)\n",
    "print('Train dataset shape: '+train.shape)\n",
    "print('Test dataset shape: '+test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef2a330",
   "metadata": {},
   "source": [
    "# Tokenization, Padding & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd6517",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b36ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()  \n",
    "tokenizer.fit_on_texts(train.content)  ### fit_on_texts is used to create the vocabulary\n",
    "vocab_size = len(tokenizer.word_index) + 1 ### vocab_size = number of unique words in the corpus\n",
    "max_length = 30 ### the longest word length in the dataset should't exceed 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87915b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "### all input sequences will be padded to have the same length\n",
    "\n",
    "sequences_train = tokenizer.texts_to_sequences(train.content) \n",
    "sequences_test = tokenizer.texts_to_sequences(test.content) \n",
    "\n",
    "xtrain = pad_sequences(sequences_train, maxlen=max_length, padding='post')\n",
    "xtest = pad_sequences(sequences_test, maxlen=max_length, padding='post')\n",
    "\n",
    "ytrain = train.target\n",
    "ytest = test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005a17e",
   "metadata": {},
   "source": [
    "# Word Embedding using Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea1a2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e7d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dictionary = dict()\n",
    "glove_file = open('glove.6B.100d.txt') ###  download file from => https://www.kaggle.com/danielwillgeorge/glove6b100dtxt <= or search for it on google\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = np.asarray(records[1:], dtype='float32')\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "    \n",
    "glove_file.close()\n",
    "\n",
    "embedding_dim = 100 # embeddings_dictionary[any Existent word].shape[0]\n",
    "\n",
    "embeddings_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embeddings_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599279a2",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe0876df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Embedding, Flatten, Conv1D, MaxPooling1D, LSTM, Conv1D, Bidirectional, Input, SpatialDropout1D\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e5e95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False)\n",
    "num_epochs = 10\n",
    "batch_size = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c904233a",
   "metadata": {},
   "source": [
    "## First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44184ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        embedding_layer,\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(LSTM(128)),\n",
    "        Dropout(0.3),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b2a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c606fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "history = model.fit(xtrain, ytrain, batch_size = batch_size, epochs=num_epochs, validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76dc44",
   "metadata": {},
   "source": [
    "## Second Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39dc167",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential([\n",
    "        embedding_layer,\n",
    "        SpatialDropout1D(0.2),\n",
    "        Conv1D(64, 5, activation='relu'),\n",
    "        Bidirectional(LSTM(128)),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dense(1, activation='sigmoid'),\n",
    "    ])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb859474",
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e1f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer=Adam(), metrics=['accuracy'])\n",
    "history = model.fit(xtrain, ytrain, batch_size = batch_size, epochs=num_epochs, validation_data=(xtest, ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab396601",
   "metadata": {},
   "source": [
    "# Evalute the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05431143",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeefe0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(ytest, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86cfdfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#History for accuracy\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train accuracy', 'Test accuracy'], loc='lower right')\n",
    "plt.show()\n",
    "# History for loss\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['Train loss', 'Test loss'], loc='upper right')\n",
    "plt.suptitle('Accuracy and loss for second model')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
