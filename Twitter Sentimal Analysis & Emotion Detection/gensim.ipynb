{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35bbb986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd93777b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv',encoding= 'latin-1',header = None)\n",
    "df = df.sample(frac = 1) ### shuffle the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "214d1e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={0: 'target', 1: 'id', 2: 'date', 3: 'query', 4: 'username', 5: 'content'}) # add names for columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3573e481",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['id','date','query','username'],axis=1) # drop unimportant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e786dc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "452826     [forwardadam, know, election, via, twitter, sa...\n",
      "1158876              [regalaffair, hey, you, are, up, early]\n",
      "21945                              [in, bed, wid, bad, cold]\n",
      "617015     [the, zoo, was, amazing, minus, the, giraffes,...\n",
      "1220931                                           [it, over]\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "### create a word vectorized model trained on our tweets so that we can use it to make the embedded layer in keras ###\n",
    "### Tokenizing tweets  ###\n",
    "import gensim  # NLP library\n",
    "tokenized_tweets = df.content.apply(gensim.utils.simple_preprocess)\n",
    "print(tokenized_tweets.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a676f485",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = gensim.models.Word2Vec(\n",
    "    window=7, # 7 words before and after the targeted word\n",
    "    min_count=2, # ignore sentences contains less than 2 words\n",
    "    workers=4, # number of cores (cpu threads)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d87a41a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model.build_vocab(tokenized_tweets,progress_per=1000) # progress_per => after how many words you want to see progress\n",
    "word2vec_model.epochs = 3 # epochs => by default they are 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba96826",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(\"gensim_model\"):\n",
    "    os.makedirs(\"gensim_model\")\n",
    "\n",
    "word2vec_model.train(tokenized_tweets,total_examples=word2vec_model.corpus_count,epochs=word2vec_model.epochs) # corpus_count = total number of sentences (tweets)\n",
    "word2vec_model.save('gensim_model\\\\VectorizedTweets.model') # save model to gensim_model folder (you can continue training later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dadc2f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "### load model ###\n",
    "from gensim.models import Word2Vec\n",
    "word2vec_model = Word2Vec.load(\"gensim_model\\\\VectorizedTweets.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d07d5cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('celebrating', 0.6401989459991455), ('janell', 0.5958845615386963), ('present', 0.5867543816566467), ('implore', 0.5770038366317749), ('blessed', 0.5501918196678162), ('pleased', 0.5448803305625916), ('presents', 0.5438183546066284), ('yuky', 0.5434587597846985), ('outragous', 0.5378299951553345), ('bash', 0.5368425846099854)]\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model.wv.most_similar(\"happy\")) # most similar words to happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53194535\n"
     ]
    }
   ],
   "source": [
    "print(word2vec_model.wv.similarity(\"happy\",\"sad\")) # similarity between happy and sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7.7209848e-01,  2.9098938e+00, -2.2881694e+00,  1.1564962e+00,\n",
       "       -7.0474006e-02,  1.9252822e+00,  2.8932507e+00, -2.0322993e+00,\n",
       "        5.2276370e-04, -1.2602391e+00, -1.0535733e+00,  1.2958065e+00,\n",
       "        2.4510903e+00,  1.4463763e+00,  2.2114964e-01,  1.3051102e+00,\n",
       "       -1.9796247e+00, -1.0180608e+00, -3.7736886e+00, -4.1302692e-02,\n",
       "       -3.5985131e+00, -9.0370959e-01,  1.3302295e+00, -2.0401471e+00,\n",
       "       -2.0792520e+00,  7.6381743e-01, -8.2763106e-01, -7.6384658e-01,\n",
       "        1.3098241e+00, -2.0987318e+00, -9.6480125e-01,  3.7115054e+00,\n",
       "       -6.6492420e-01,  1.1560215e+00, -2.0045030e+00,  9.9720895e-01,\n",
       "       -2.7270319e+00, -3.6406973e-01, -3.1315205e+00,  5.2508420e-01,\n",
       "        2.0887537e-01,  2.4835315e+00, -9.7937882e-01, -3.6917013e-01,\n",
       "        1.8493687e+00, -2.1091148e-01, -7.5071257e-01,  1.3299061e+00,\n",
       "        8.9197558e-01,  9.8119003e-01, -1.4542979e-02, -4.9188328e-01,\n",
       "       -6.3787031e-01, -3.6916361e+00, -1.0722351e+00, -2.9612443e+00,\n",
       "        1.2671807e+00,  1.9855409e+00,  2.9452515e+00,  7.4750078e-01,\n",
       "        5.9242481e-01,  2.7493300e+00, -1.0107847e+00,  5.7005489e-01,\n",
       "       -3.4849420e+00,  4.6437949e-01,  9.5874560e-01, -9.7451013e-01,\n",
       "       -3.5813553e+00,  2.4507057e-02,  7.9813081e-01,  1.4073124e+00,\n",
       "        1.1782234e+00, -9.9628419e-01, -1.4844301e-01,  7.9301196e-01,\n",
       "       -1.1822071e+00, -2.4958367e+00, -8.2416274e-02,  8.1213510e-01,\n",
       "        1.5475594e+00,  2.0883019e+00,  1.5417705e+00,  3.0241314e-01,\n",
       "        8.9251125e-01, -2.0707011e+00, -8.7572300e-01,  1.7336191e+00,\n",
       "        1.2253897e+00, -2.6981468e+00, -5.6039184e-01,  2.6564491e+00,\n",
       "        2.3711965e+00, -6.3166785e-01, -2.0557015e-01,  5.3306270e-01,\n",
       "        1.9784601e+00, -3.2478709e-02, -3.4641480e+00,  1.7174748e+00],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv[\"happy\"] # get word vector of happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to\n",
      "the\n",
      "my\n"
     ]
    }
   ],
   "source": [
    "# get the most common 3 words\n",
    "print(word2vec_model.wv.index_to_key[0])\n",
    "print(word2vec_model.wv.index_to_key[1])\n",
    "print(word2vec_model.wv.index_to_key[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kenno\n",
      "jay_rachinea\n",
      "splashin\n"
     ]
    }
   ],
   "source": [
    "# get the least common 3 words\n",
    "print(word2vec_model.wv.index_to_key[-1])\n",
    "print(word2vec_model.wv.index_to_key[-2])\n",
    "print(word2vec_model.wv.index_to_key[-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.key_to_index[\"happy\"] # get the index of happy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'zebra'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.doesnt_match([\"green\", \"blue\", \"red\", \"zebra\"]) # get the word that is different from other words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec as Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.40618587e+00, -4.43708986e-01, -5.81877947e-01, ...,\n",
       "        -1.88972306e+00,  4.82971877e-01, -1.30342066e+00],\n",
       "       [ 2.72394753e+00,  9.42519784e-01,  1.34451652e+00, ...,\n",
       "        -1.26569605e+00,  1.64840174e+00, -3.77694488e+00],\n",
       "       [ 3.01912665e+00, -4.37624514e-01, -2.86220026e+00, ...,\n",
       "        -2.50975728e+00,  2.63309591e-02, -4.38553762e+00],\n",
       "       ...,\n",
       "       [-1.85065977e-02,  3.52748968e-02,  2.82073170e-02, ...,\n",
       "        -1.62989236e-02, -5.30586578e-03, -1.60317123e-02],\n",
       "       [-1.05304271e-02, -4.01071506e-03, -1.05484780e-02, ...,\n",
       "        -8.12941231e-03, -1.15895560e-02,  2.96911894e-04],\n",
       "       [-2.13941233e-03,  1.18429000e-02, -1.47233577e-02, ...,\n",
       "        -3.04355994e-02, -2.11580135e-02, -3.12503800e-02]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the wv word vector into a numpy matrix that is suitable for keras\n",
    "# insertion into TensorFlow and Keras models\n",
    "import numpy as np\n",
    "embedding_matrix = np.zeros((len(word2vec_model.wv.key_to_index),word2vec_model.vector_size))\n",
    "for i in range(len(word2vec_model.wv.key_to_index)):\n",
    "    embedding_vector = word2vec_model.wv[word2vec_model.wv.index_to_key[i]]\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getMaxWordLength():\n",
    "    max_word_length = 0\n",
    "    for i in word2vec_model.wv.key_to_index.keys():\n",
    "        if len(i) > max_word_length:\n",
    "            max_word_length = len(i)\n",
    "    return max_word_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec_model.wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from  tensorflow.keras.layers import Embedding, Input\n",
    "MAX_SEQUENCE_LENGTH = getMaxWordLength()\n",
    "embedding_layer = tf.keras.layers.Embedding(word2vec_model.wv.vector_size, \n",
    "                                            input_dim=embedding_matrix.shape[0], \n",
    "                                            input_length = MAX_SEQUENCE_LENGTH, \n",
    "                                            weights=[embedding_matrix], \n",
    "                                            trainable=False)\n",
    "sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedding_layer = embedding_layer(sequence_input)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c07329831e65ccf6201e6c731d584b258e5a47eb14b880d3b7981c10572e274f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
